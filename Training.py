import torch
import torch.backends.cudnn as cudnn
import torch.nn as nn
from torch import optim
from torch.autograd import Variable
import torch.multiprocessing as mp
import torch.distributed as dist
# from apex import amp

from dataset import get_loader
import math
from Models.ImageDepthNet import ImageDepthNet
import os
import torch.nn.functional as F

import numpy as np

def structure_loss(pred, mask):
    weit = 1 + 5*torch.abs(F.avg_pool2d(mask, kernel_size=31, stride=1, padding=15) - mask)
    wbce = F.binary_cross_entropy_with_logits(pred, mask, reduction='mean')
    wbce = (weit*wbce).sum(dim=(2, 3)) / weit.sum(dim=(2, 3))

    pred = torch.sigmoid(pred)
    inter = ((pred * mask)*weit).sum(dim=(2, 3))
    union = ((pred + mask)*weit).sum(dim=(2, 3))
    wiou = 1 - (inter + 1)/(union - inter+1)
    return (wbce + wiou).mean()

def iou_loss(pred, mask):
    pred  = torch.sigmoid(pred)
    inter = (pred*mask).sum(dim=(2,3))
    union = (pred+mask).sum(dim=(2,3))
    iou  = 1-(inter+1)/(union-inter+1)
    return iou.mean()

def eval_pr(y_pred, y, num):
    prec, recall = torch.zeros(num).cuda(), torch.zeros(num).cuda()
    # thlist = torch.linspace(0, 1 - 1e-10, num).cuda()
    thlist = torch.linspace(0, 1, num).cuda()

    for i in range(num-1):
        y_temp = torch.logical_and(y_pred>= thlist[i], y_pred < thlist[i+1]).float()
        #y_temp = (y_pred >= thlist[i] ).float()
        tp = (y_temp * y).sum()
        prec[i], recall[i] = tp / (y_temp.sum() + 1e-20), tp / (y.sum() + 1e-20)
    return prec, recall

def pr_loss(pred, gt):
    pred = torch.sigmoid(pred)
    prec, recall = eval_pr(pred, gt, 255)
    prec_loss = 1.0 - prec
    recall_loss = 1.0 - recall
    prec_loss = prec_loss.mean()
    recall_loss = recall_loss.mean()

    loss = prec_loss + recall_loss

    return  loss

def object(pred, gt) -> float:
    """
    Calculate the object score.
    """
    fg = pred * gt
    bg = (1 - pred) * (1 - gt)
    u = torch.mean(gt)
    object_score = u * s_object(fg, gt) + (1 - u) * s_object(bg, 1 - gt)
    return object_score

_EPS = np.spacing(1)
def s_object(pred, gt) -> float:
    x = torch.mean(pred[gt == 1])
    sigma_x = torch.std(pred[gt == 1], unbiased=True)
    score = 2 * x / (torch.pow(x, 2) + 1 + sigma_x + _EPS)
    return score

def region( pred, gt) -> float:
    """
    Calculate the region score.
    """
    x, y = centroid(gt)
    part_info = divide_with_xy(pred, gt, x, y)
    w1, w2, w3, w4 = part_info["weight"]
    # assert np.isclose(w1 + w2 + w3 + w4, 1), (w1 + w2 + w3 + w4, pred.mean(), gt.mean())

    pred1, pred2, pred3, pred4 = part_info["pred"]
    gt1, gt2, gt3, gt4 = part_info["gt"]
    score1 = ssim(pred1, gt1)
    score2 = ssim(pred2, gt2)
    score3 = ssim(pred3, gt3)
    score4 = ssim(pred4, gt4)

    return w1 * score1 + w2 * score2 + w3 * score3 + w4 * score4

def centroid(matrix) -> tuple:
    """
    To ensure consistency with the matlab code, one is added to the centroid coordinate,
    so there is no need to use the redundant addition operation when dividing the region later,
    because the sequence generated by ``1:X`` in matlab will contain ``X``.

    :param matrix: a data array
    :return: the centroid coordinate
    """
    h, w = matrix.shape
    if matrix.sum() == 0:
        x = torch.round(w / 2)
        y = torch.round(h / 2)
    else:
        area_object = torch.sum(matrix)
        row_ids = torch.arange(h).cuda()
        col_ids = torch.arange(w).cuda()
        x = torch.round(torch.sum(torch.sum(matrix, axis=0) * col_ids) / area_object)
        y = torch.round(torch.sum(torch.sum(matrix, axis=1) * row_ids) / area_object)
    return int(x) + 1, int(y) + 1

def divide_with_xy(pred, gt, x: int, y: int) -> dict:
    """
    Use (x,y) to divide the ``pred`` and the ``gt`` into four submatrices, respectively.
    """
    h, w = gt.shape
    area = h * w

    gt_LT = gt[0:y, 0:x]
    gt_RT = gt[0:y, x:w]
    gt_LB = gt[y:h, 0:x]
    gt_RB = gt[y:h, x:w]

    pred_LT = pred[0:y, 0:x]
    pred_RT = pred[0:y, x:w]
    pred_LB = pred[y:h, 0:x]
    pred_RB = pred[y:h, x:w]

    w1 = x * y / area
    w2 = y * (w - x) / area
    w3 = (h - y) * x / area
    w4 = 1 - w1 - w2 - w3

    return dict(
        gt=(gt_LT, gt_RT, gt_LB, gt_RB),
        pred=(pred_LT, pred_RT, pred_LB, pred_RB),
        weight=(w1, w2, w3, w4),
    )

def ssim(pred, gt) -> float:
    """
    Calculate the ssim score.
    """
    h, w = pred.shape
    N = h * w

    x = torch.mean(pred)
    y = torch.mean(gt)

    sigma_x = torch.sum((pred - x) ** 2) / (N - 1)
    sigma_y = torch.sum((gt - y) ** 2) / (N - 1)
    sigma_xy = torch.sum((pred - x) * (gt - y)) / (N - 1)

    alpha = 4 * x * y * sigma_xy
    beta = (x ** 2 + y ** 2) * (sigma_x + sigma_y)

    if alpha != 0:
        score = alpha / (beta + _EPS)
    elif alpha == 0 and beta == 0:
        score = 1
    else:
        score = 0
    return score


def Sm(pred, gt):
    """
    Calculate the S-measure.

    :return: s-measure
    """
    alpha = 0.5
    y = torch.mean(gt)
    if y == 0:
        sm = 1 - torch.mean(pred)
    elif y == 1:
        sm = torch.mean(pred)
    else:
        sm = alpha * object(pred, gt) + (1 - alpha) * region(pred, gt)
        sm = max(0, sm)
    return sm


def Sm_loss1(pred, mask):
    sm = 0.0
    pred = torch.sigmoid(pred)
    for i in range(pred.shape[0]):
        sm += Sm( pred=pred[i,0,:], gt=mask[i,0,:])
    sm = sm / pred.shape[0]
    sm_loss = 1.0 - sm
    return sm_loss


def save_loss(save_dir, whole_iter_num, epoch_total_loss, epoch_loss, epoch):
    fh = open(save_dir, 'a')
    epoch_total_loss = str(epoch_total_loss)
    epoch_loss = str(epoch_loss)
    fh.write('until_' + str(epoch) + '_run_iter_num' + str(whole_iter_num) + '\n')
    fh.write(str(epoch) + '_epoch_total_loss' + epoch_total_loss + '\n')
    fh.write(str(epoch) + '_epoch_loss' + epoch_loss + '\n')
    fh.write('\n')
    fh.close()


def adjust_learning_rate(optimizer, decay_rate=.1):
    update_lr_group = optimizer.param_groups
    for param_group in update_lr_group:
        print('before lr: ', param_group['lr'])
        param_group['lr'] = param_group['lr'] * decay_rate
        print('after lr: ', param_group['lr'])
    return optimizer


def save_lr(save_dir, optimizer):
    update_lr_group = optimizer.param_groups[0]
    fh = open(save_dir, 'a')
    fh.write('encode:update:lr' + str(update_lr_group['lr']) + '\n')
    fh.write('decode:update:lr' + str(update_lr_group['lr']) + '\n')
    fh.write('\n')
    fh.close()


def train_net(num_gpus, args):

    mp.spawn(main, nprocs=num_gpus, args=(num_gpus, args))


def main(local_rank, num_gpus, args):

    cudnn.benchmark = True

    dist.init_process_group(backend='nccl', init_method=args.init_method, world_size=num_gpus, rank=local_rank)

    torch.cuda.set_device(local_rank)

    net = ImageDepthNet(args)
    net.train()
    net.cuda()

    net = nn.SyncBatchNorm.convert_sync_batchnorm(net)
    net = torch.nn.parallel.DistributedDataParallel(
        net,
        device_ids=[local_rank],
        output_device=local_rank,
        find_unused_parameters=True)

    base_params = [params for name, params in net.named_parameters() if ("backbone" in name)]
    other_params = [params for name, params in net.named_parameters() if ("backbone" not in name)]

    optimizer = optim.Adam([{'params': base_params, 'lr': args.lr * 0.1},
                            {'params': other_params, 'lr': args.lr}])
    # net, optimizer = amp.initialize(net, optimizer, opt_level="O1")

    train_dataset = get_loader(args.trainset, args.data_root, args.img_size, mode='train')

    sampler = torch.utils.data.distributed.DistributedSampler(
        train_dataset,
        num_replicas=num_gpus,
        rank=local_rank,
    )
    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=args.batch_size, num_workers=6,
                                               pin_memory=True,
                                               sampler=sampler,
                                               drop_last=True,
                                               )

    print('''
        Starting training:
            Train steps: {}
            Batch size: {}
            Learning rate: {}
            Training size: {}
        '''.format(args.train_steps, args.batch_size, args.lr, len(train_loader.dataset)))

    N_train = len(train_loader) * args.batch_size

    loss_weights = [1, 0.8, 0.8, 0.5, 0.5, 0.5]
    if not os.path.exists(args.save_model_dir):
        os.makedirs(args.save_model_dir)

    criterion = nn.BCEWithLogitsLoss()
    whole_iter_num = 0
    iter_num = math.ceil(len(train_loader.dataset) / args.batch_size)
   
    for epoch in range(args.epochs):

        print('Starting epoch {}/{}.'.format(epoch + 1, args.epochs))
        print('epoch:{0}-------lr:{1}'.format(epoch + 1, args.lr))

        epoch_total_loss = 0
        epoch_loss = 0

        for i, data_batch in enumerate(train_loader):
            if (i + 1) > iter_num: break

            images, label_224, label_14, label_28, label_56, label_112, \
            contour_224, contour_14, contour_28, contour_56, contour_112 = data_batch

            images, label_224, contour_224 = Variable(images.cuda(local_rank, non_blocking=True)), \
                                        Variable(label_224.cuda(local_rank, non_blocking=True)),  \
                                        Variable(contour_224.cuda(local_rank, non_blocking=True))

            label_14, label_28, label_56, label_112 = Variable(label_14.cuda()), Variable(label_28.cuda()),\
                                                      Variable(label_56.cuda()), Variable(label_112.cuda())

            contour_14, contour_28, contour_56, contour_112 = Variable(contour_14.cuda()), \
                                                                                      Variable(contour_28.cuda()), \
                                                      Variable(contour_56.cuda()), Variable(contour_112.cuda())

            outputs_saliency, outputs_contour = net(images)

            mask_1_16, mask_1_8, mask_1_4, mask_1_1 = outputs_saliency
            cont_1_16, cont_1_8, cont_1_4, cont_1_1 = outputs_contour
            # saliency loss
            # loss5 = criterion(mask_1_16, label_14)
            # loss4 = criterion(mask_1_8, label_28)
            # loss3 = criterion(mask_1_4, label_56)
            # loss1 = criterion(mask_1_1, label_224)
            loss5 = F.binary_cross_entropy_with_logits(mask_1_16, label_14) + iou_loss(mask_1_16, label_14) + structure_loss(mask_1_16, label_14)
            loss4 = F.binary_cross_entropy_with_logits(mask_1_8, label_28) + iou_loss(mask_1_8, label_28) + structure_loss(mask_1_8, label_28)
            loss3 = F.binary_cross_entropy_with_logits(mask_1_4, label_56) + iou_loss(mask_1_4, label_56) + structure_loss(mask_1_4, label_56)
            loss1 = F.binary_cross_entropy_with_logits(mask_1_1, label_224) + iou_loss(mask_1_1, label_224) + structure_loss(mask_1_1, label_224)  + Sm_loss1(mask_1_1,label_224)

            # contour loss
            # c_loss5 = criterion(cont_1_16, contour_14)
            # c_loss4 = criterion(cont_1_8, contour_28)
            # c_loss3 = criterion(cont_1_4, contour_56)
            # c_loss1 = criterion(cont_1_1, contour_224)

            c_loss5 = F.binary_cross_entropy_with_logits(cont_1_16, contour_14) + iou_loss(cont_1_16, contour_14) + structure_loss(cont_1_16, contour_14)
            c_loss4 = F.binary_cross_entropy_with_logits(cont_1_4, contour_56) + iou_loss(cont_1_4, contour_56) + structure_loss(cont_1_4, contour_56)
            c_loss3 = F.binary_cross_entropy_with_logits(cont_1_8, contour_28) + iou_loss(cont_1_8, contour_28) + structure_loss(cont_1_8, contour_28)
            c_loss1 = F.binary_cross_entropy_with_logits(cont_1_1, contour_224) + iou_loss(cont_1_1, contour_224) + structure_loss(cont_1_1, contour_224) + Sm_loss1(cont_1_1,contour_224)

            img_total_loss = loss_weights[0] * loss1 + loss_weights[2] * loss3 + loss_weights[3] * loss4 + loss_weights[4] * loss5
            contour_total_loss = loss_weights[0] * c_loss1 + loss_weights[2] * c_loss3 + loss_weights[3] * c_loss4 + loss_weights[4] * c_loss5

            total_loss = img_total_loss + contour_total_loss

            epoch_total_loss += total_loss.cpu().data.item()
            epoch_loss += loss1.cpu().data.item()

            print(
                'whole_iter_num: {0} --- {1:.4f} --- total_loss: {2:.6f} --- saliency loss: {3:.6f}'.format(
                    (whole_iter_num + 1),
                    (i + 1) * args.batch_size / N_train, total_loss.item(), loss1.item()))

            optimizer.zero_grad()
            total_loss.backward()

            optimizer.step()
            whole_iter_num += 1



            if (local_rank == 0) and (whole_iter_num == 45000):
                torch.save(net.state_dict(), args.save_model_dir + '45000.pth')

            if (local_rank == 0) and (whole_iter_num == 50000):
                torch.save(net.state_dict(), args.save_model_dir + '50000.pth')

            if (local_rank == 0) and (whole_iter_num == 55000):
                torch.save(net.state_dict(), args.save_model_dir + '55000.pth')

            if (local_rank == 0) and (whole_iter_num == args.train_steps):
                torch.save(net.state_dict(), args.save_model_dir + 'ERNet.pth')

            if whole_iter_num == args.train_steps:
                return 0

            if whole_iter_num == args.stepvalue1 or whole_iter_num == args.stepvalue2:
                optimizer = adjust_learning_rate(optimizer, decay_rate=args.lr_decay_gamma)
                save_dir = './loss.txt'
                save_lr(save_dir, optimizer)
                print('have updated lr!!')

        print('Epoch finished ! Loss: {}'.format(epoch_total_loss / iter_num))
        save_lossdir = './loss.txt'
        save_loss(save_lossdir, whole_iter_num, epoch_total_loss / iter_num, epoch_loss/iter_num, epoch+1)






